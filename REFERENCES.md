# References

<div id="refs" class="references csl-bib-body">

<div id="ref-schoenholz2017deepinformationpropagation"
class="csl-entry">

<span class="csl-left-margin">\[1\] </span><span
class="csl-right-inline">S. S. Schoenholz, J. Gilmer, S. Ganguli, and J.
Sohl-Dickstein, “Deep information propagation.” 2017. Available:
<https://arxiv.org/abs/1611.01232></span>

</div>

<div id="ref-poole2016exponentialexpressivitydeepneural"
class="csl-entry">

<span class="csl-left-margin">\[2\] </span><span
class="csl-right-inline">B. Poole, S. Lahiri, M. Raghu, J.
Sohl-Dickstein, and S. Ganguli, “Exponential expressivity in deep neural
networks through transient chaos.” 2016. Available:
<https://arxiv.org/abs/1606.05340></span>

</div>

<div id="ref-cowsik2024geometricdynamicssignalpropagation"
class="csl-entry">

<span class="csl-left-margin">\[3\] </span><span
class="csl-right-inline">A. Cowsik, T. Nebabu, X.-L. Qi, and S. Ganguli,
“Geometric dynamics of signal propagation predict trainability of
transformers.” 2024. Available:
<https://arxiv.org/abs/2403.02579></span>

</div>

<div id="ref-doshi2023criticalinitializationwidedeep" class="csl-entry">

<span class="csl-left-margin">\[4\] </span><span
class="csl-right-inline">D. Doshi, T. He, and A. Gromov, “Critical
initialization of wide and deep neural networks through partial
jacobians: General theory and applications.” 2023. Available:
<https://arxiv.org/abs/2111.12143></span>

</div>

<div id="ref-kedia2024transformersstableendtoendsignal"
class="csl-entry">

<span class="csl-left-margin">\[5\] </span><span
class="csl-right-inline">A. Kedia, M. A. Zaidi, S. Khyalia, J. Jung, H.
Goka, and H. Lee, “Transformers get stable: An end-to-end signal
propagation theory for language models.” 2024. Available:
<https://arxiv.org/abs/2403.09635></span>

</div>

<div id="ref-wang2022deepnetscalingtransformers1000" class="csl-entry">

<span class="csl-left-margin">\[6\] </span><span
class="csl-right-inline">H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang,
and F. Wei, “DeepNet: Scaling transformers to 1,000 layers.” 2022.
Available: <https://arxiv.org/abs/2203.00555></span>

</div>

<div id="ref-kim2025perilnrevisitingnormalizationlayer"
class="csl-entry">

<span class="csl-left-margin">\[7\] </span><span
class="csl-right-inline">J. Kim *et al.*, “Peri-LN: Revisiting
normalization layer in the transformer architecture.” 2025. Available:
<https://arxiv.org/abs/2502.02732></span>

</div>

<div id="ref-oh2025revisitingresidualconnectionsorthogonal"
class="csl-entry">

<span class="csl-left-margin">\[8\] </span><span
class="csl-right-inline">G. Oh, W. Cho, S. Kim, S. Choi, and Y. Yu,
“Revisiting residual connections: Orthogonal updates for stable and
efficient deep networks.” 2025. Available:
<https://arxiv.org/abs/2505.11881></span>

</div>

<div id="ref-zhang2025grouprepresentationalpositionencoding"
class="csl-entry">

<span class="csl-left-margin">\[9\] </span><span
class="csl-right-inline">Y. Zhang *et al.*, “Group representational
position encoding.” 2025. Available:
<https://arxiv.org/abs/2512.07805></span>

</div>

<div id="ref-xiong2020layernormalizationtransformerarchitecture"
class="csl-entry">

<span class="csl-left-margin">\[10\] </span><span
class="csl-right-inline">R. Xiong *et al.*, “On layer normalization in
the transformer architecture.” 2020. Available:
<https://arxiv.org/abs/2002.04745></span>

</div>

<div id="ref-zhang2019rootmeansquarelayer" class="csl-entry">

<span class="csl-left-margin">\[11\] </span><span
class="csl-right-inline">B. Zhang and R. Sennrich, “Root mean square
layer normalization.” 2019. Available:
<https://arxiv.org/abs/1910.07467></span>

</div>

</div>
