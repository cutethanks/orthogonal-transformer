{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs: set your checkpoint directory and step\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "from model import GPT, GPTConfig\n",
        "\n",
        "# EDIT these two\n",
        "CKPT_DIR = '/content/drive/MyDrive/ml_projects/post_rmsnorm/out-small-model-post/lr_0.001'\n",
        "STEP = 5000  # e.g., 5000\n",
        "\n",
        "# Device & dtype\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint and construct the model\n",
        "import sys\n",
        "\n",
        "ckpt_path_step = os.path.join(CKPT_DIR, 'checkpoints', f'ckpt_{STEP}.pt')\n",
        "ckpt_path_latest = os.path.join(CKPT_DIR, 'ckpt.pt')\n",
        "ckpt_path = ckpt_path_step if os.path.exists(ckpt_path_step) else ckpt_path_latest\n",
        "print('Loading checkpoint from:', ckpt_path)\n",
        "\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "model_args = dict(checkpoint['model_args'])  # base model args saved during training\n",
        "cfg_dict = checkpoint.get('config', {})      # full training config (for extra flags)\n",
        "\n",
        "# Ensure normalization flags are consistent with training\n",
        "for k in ['post_ln', 'rmsnorm', 'ln_learnable']:\n",
        "    if k in cfg_dict:\n",
        "        model_args[k] = cfg_dict[k]\n",
        "\n",
        "# Build model and load weights\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "# fix any unwanted prefixes\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval().to(device)\n",
        "\n",
        "print('Model loaded. Iteration:', checkpoint.get('iter_num'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build tokenizer (char-level). Try to load meta.pkl from training dataset\n",
        "import json\n",
        "\n",
        "dataset = cfg_dict.get('dataset', 'openwebtext')\n",
        "meta_path = os.path.join('data', dataset, 'meta.pkl')\n",
        "print('Dataset:', dataset)\n",
        "print('Looking for meta at:', meta_path)\n",
        "\n",
        "stoi = itos = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    stoi = meta.get('stoi')\n",
        "    itos = meta.get('itos')\n",
        "    print('Loaded vocabulary from meta.pkl (size = {})'.format(len(itos) if itos else 'unknown'))\n",
        "\n",
        "if stoi is not None and itos is not None:\n",
        "    def encode(s: str):\n",
        "        return [stoi[ch] for ch in s]\n",
        "    def decode(tokens):\n",
        "        return ''.join([itos[i] for i in tokens])\n",
        "else:\n",
        "    # Fallback: byte-level encode/decode (works without meta)\n",
        "    def encode(s: str):\n",
        "        return list(s.encode('utf-8'))\n",
        "    def decode(tokens):\n",
        "        return bytes(tokens).decode('utf-8', errors='ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation using the model's built-in generate()\n",
        "prompt = \"To be, or not to be\"\n",
        "max_new_tokens = 200\n",
        "temperature = 1.0\n",
        "TopK = 50  # set to None to disable top-k filtering\n",
        "\n",
        "# Encode prompt and generate\n",
        "idx = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "idx = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=TopK)\n",
        "\n",
        "# Decode and print\n",
        "print(decode(idx[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a ~200-char Shakespeare sample and tokenize\n",
        "sample_text = (\n",
        "    \"To be, or not to be, that is the question:\\n\"\n",
        "    \"Whether 'tis nobler in the mind to suffer\\n\"\n",
        "    \"The slings and arrows of outrageous fortune,\\n\"\n",
        "    \"Or to take arms against a sea of troubles\\n\"\n",
        ")\n",
        "idx_sample = torch.tensor([encode(sample_text)], dtype=torch.long, device=device)\n",
        "S = idx_sample.size(1)\n",
        "print('Sequence length S =', S)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capture per-layer activations with forward hooks and prepend embeddings\n",
        "block_outputs = []\n",
        "hooks = []\n",
        "\n",
        "# register hooks on each transformer block to capture its output (shape: 1 x S x d)\n",
        "for i, block in enumerate(model.transformer.h):\n",
        "    def make_hook(i):\n",
        "        def hook(module, inp, out):\n",
        "            block_outputs.append(out.detach())\n",
        "        return hook\n",
        "    hooks.append(block.register_forward_hook(make_hook(i)))\n",
        "\n",
        "# run a single forward pass (no targets, no generation)\n",
        "with torch.no_grad():\n",
        "    _logits, _ = model(idx_sample)\n",
        "\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n",
        "\n",
        "# stack to L x S x d\n",
        "layer_acts = torch.stack([t.squeeze(0) for t in block_outputs], dim=0)\n",
        "\n",
        "# compute embedding + positional (and ln_emb if post_ln)\n",
        "with torch.no_grad():\n",
        "    tok_emb = model.transformer.wte(idx_sample)              # 1 x S x d\n",
        "    pos = torch.arange(0, S, dtype=torch.long, device=device)\n",
        "    pos_emb = model.transformer.wpe(pos)                     # S x d\n",
        "    x0 = tok_emb + pos_emb\n",
        "    if getattr(model.config, 'post_ln', False):\n",
        "        x0 = model.transformer.ln_emb(x0)\n",
        "    x0 = x0.squeeze(0)                                       # S x d\n",
        "\n",
        "# final array: (L+1) x S x d\n",
        "all_acts = torch.cat([x0.unsqueeze(0), layer_acts], dim=0)\n",
        "print('all_acts shape:', tuple(all_acts.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average L2 norm per layer (averaged over token positions)\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "assert 'all_acts' in globals(), \"all_acts (L+1, S, d) not found\"\n",
        "Lp1, S, d = all_acts.shape\n",
        "\n",
        "with torch.no_grad():\n",
        "    norms = all_acts.norm(dim=-1)               # (L+1, S)\n",
        "    avg_norm = norms.mean(dim=1).cpu().numpy()  # (L+1,)\n",
        "\n",
        "layers = list(range(Lp1))\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(layers, avg_norm, marker='o')\n",
        "plt.xlabel('layer (0 = embeddings)')\n",
        "plt.ylabel('avg ||activation||')\n",
        "plt.title('Average activation norm vs. layer')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average cosine similarity between consecutive layers (layer vs. layer-1)\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "eps = 1e-8\n",
        "# a: layers 1..L (exclude embeddings), b: layers 0..L-1\n",
        "a = all_acts[1:]     # (L, S, d)\n",
        "b = all_acts[:-1]    # (L, S, d)\n",
        "\n",
        "with torch.no_grad():\n",
        "    dot = (a * b).sum(dim=-1)                         # (L, S)\n",
        "    na  = a.norm(dim=-1).clamp_min(eps)               # (L, S)\n",
        "    nb  = b.norm(dim=-1).clamp_min(eps)               # (L, S)\n",
        "    cos = (dot / (na * nb)).mean(dim=1).cpu().numpy() # (L,)\n",
        "\n",
        "layers = list(range(1, all_acts.size(0)))\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(layers, cos, marker='o', color='tab:orange')\n",
        "plt.xlabel('layer')\n",
        "plt.ylabel('avg cos(layer, layer-1)')\n",
        "plt.title('Average cosine similarity vs. layer')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache c_proj outputs (\"velocities\") for attention and MLP per layer\n",
        "attn_vel_outs, mlp_vel_outs = [], []\n",
        "attn_hooks, mlp_hooks = [], []\n",
        "\n",
        "for i, block in enumerate(model.transformer.h):\n",
        "    # attention c_proj output\n",
        "    def make_attn_hook(i):\n",
        "        def hook(module, inp, out):\n",
        "            attn_vel_outs.append(out.detach())  # shape: 1 x S x d\n",
        "        return hook\n",
        "    attn_hooks.append(block.attn.c_proj.register_forward_hook(make_attn_hook(i)))\n",
        "\n",
        "    # mlp c_proj output\n",
        "    def make_mlp_hook(i):\n",
        "        def hook(module, inp, out):\n",
        "            mlp_vel_outs.append(out.detach())   # shape: 1 x S x d\n",
        "        return hook\n",
        "    mlp_hooks.append(block.mlp.c_proj.register_forward_hook(make_mlp_hook(i)))\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model(idx_sample)\n",
        "\n",
        "for h in attn_hooks + mlp_hooks:\n",
        "    h.remove()\n",
        "\n",
        "# Stack to (L, S, d)\n",
        "attn_vel = torch.stack([t.squeeze(0) for t in attn_vel_outs], dim=0)\n",
        "mlp_vel  = torch.stack([t.squeeze(0) for t in mlp_vel_outs],  dim=0)\n",
        "print('attn_vel:', tuple(attn_vel.shape), 'mlp_vel:', tuple(mlp_vel.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cosine similarity: velocity vs. activation at the same layer (averaged over tokens)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "eps = 1e-8\n",
        "assert 'layer_acts' in globals(), \"layer_acts (L, S, d) not found; run the hooks cell that builds it\"\n",
        "\n",
        "# Ensure shapes match\n",
        "L = layer_acts.size(0)\n",
        "assert attn_vel.size(0) == L and mlp_vel.size(0) == L\n",
        "\n",
        "with torch.no_grad():\n",
        "    # attention\n",
        "    dot_a = (attn_vel * layer_acts).sum(dim=-1)              # (L, S)\n",
        "    na = attn_vel.norm(dim=-1).clamp_min(eps)                # (L, S)\n",
        "    nx = layer_acts.norm(dim=-1).clamp_min(eps)              # (L, S)\n",
        "    cos_attn = (dot_a / (na * nx)).mean(dim=1).cpu().numpy() # (L,)\n",
        "\n",
        "    # mlp\n",
        "    dot_m = (mlp_vel * layer_acts).sum(dim=-1)               # (L, S)\n",
        "    nm = mlp_vel.norm(dim=-1).clamp_min(eps)                 # (L, S)\n",
        "    cos_mlp = (dot_m / (nm * nx)).mean(dim=1).cpu().numpy()  # (L,)\n",
        "\n",
        "layers = list(range(1, L+1))  # plot layers starting at 1; embeddings are layer 0 in other plots\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(layers, cos_attn, marker='o', label='attn velocity ⋅ activation')\n",
        "plt.plot(layers, cos_mlp,  marker='o', label='mlp velocity ⋅ activation')\n",
        "plt.xlabel('layer index (1..L)')\n",
        "plt.ylabel('avg cosine similarity')\n",
        "plt.title('Cosine similarity: velocity vs activation (per layer)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap: cosine similarity between consecutive layers per token (L x S)\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "eps = 1e-8\n",
        "# a: layers 1..L (exclude embeddings), b: layers 0..L-1\n",
        "a = all_acts[1:]     # (L, S, d)\n",
        "b = all_acts[:-1]    # (L, S, d)\n",
        "\n",
        "with torch.no_grad():\n",
        "    dot = (a * b).sum(dim=-1)                # (L, S)\n",
        "    na  = a.norm(dim=-1).clamp_min(eps)      # (L, S)\n",
        "    nb  = b.norm(dim=-1).clamp_min(eps)      # (L, S)\n",
        "    cos_map = (dot / (na * nb)).cpu().numpy()  # (L, S)\n",
        "\n",
        "L, S = cos_map.shape\n",
        "plt.figure(figsize=(10, 5))\n",
        "im = plt.imshow(cos_map, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.colorbar(im, label='cosine similarity')\n",
        "plt.xlabel('token position (0..S-1)')\n",
        "plt.ylabel('layer (1..L)')\n",
        "plt.yticks(ticks=range(L), labels=[str(i) for i in range(1, L+1)])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('Cosine similarity heatmap: layer vs layer-1 per token')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Draw a training batch and run forward pass to compute loss\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Configure batch size here\n",
        "BATCH_SIZE = 8  # <-- change as needed\n",
        "BLOCK_SIZE = gptconf.block_size\n",
        "\n",
        "# Dataset memmaps (same source as training)\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_mm = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_train_batch(batch_size=BATCH_SIZE, block_size=BLOCK_SIZE):\n",
        "    ix = torch.randint(len(train_mm) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((train_mm[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((train_mm[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Switch to train mode (if you want dropout etc.); gradients will be computed anyway\n",
        "model.train()\n",
        "Xb, Yb = get_train_batch()\n",
        "model.zero_grad(set_to_none=True)\n",
        "logits, loss = model(Xb, Yb)\n",
        "print('Batch:', Xb.shape, 'Loss:', float(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register hooks to cache inputs to RMSNorm modules and their gradients\n",
        "from collections import OrderedDict\n",
        "from model import RMSNorm as _RMSNorm\n",
        "\n",
        "ln_in_tensors = OrderedDict()\n",
        "ln_hooks = []\n",
        "\n",
        "# helper to register a hook capturing the module input (first arg)\n",
        "def _make_ln_hook(name):\n",
        "    def hook(module, inp, out):\n",
        "        x_in = inp[0]\n",
        "        # retain grad for non-leaf tensors so we can read x_in.grad after backward\n",
        "        x_in.retain_grad()\n",
        "        ln_in_tensors[name] = x_in\n",
        "    return hook\n",
        "\n",
        "# ln_emb (post-layernorm models only)\n",
        "if hasattr(model.transformer, 'ln_emb') and isinstance(model.transformer.ln_emb, _RMSNorm):\n",
        "    ln_hooks.append(model.transformer.ln_emb.register_forward_hook(_make_ln_hook('ln_emb_in')))\n",
        "\n",
        "# per-block ln_1 and ln_2\n",
        "for li, block in enumerate(model.transformer.h):\n",
        "    if isinstance(block.ln_1, _RMSNorm):\n",
        "        ln_hooks.append(block.ln_1.register_forward_hook(_make_ln_hook(f'block{li}.ln_1_in')))\n",
        "    if isinstance(block.ln_2, _RMSNorm):\n",
        "        ln_hooks.append(block.ln_2.register_forward_hook(_make_ln_hook(f'block{li}.ln_2_in')))\n",
        "\n",
        "# final ln_f\n",
        "if isinstance(model.transformer.ln_f, _RMSNorm):\n",
        "    ln_hooks.append(model.transformer.ln_f.register_forward_hook(_make_ln_hook('ln_f_in')))\n",
        "\n",
        "# Run a forward+backward to populate grads\n",
        "logits, loss = model(Xb, Yb)\n",
        "loss.backward()\n",
        "\n",
        "# Remove hooks\n",
        "for h in ln_hooks:\n",
        "    h.remove()\n",
        "\n",
        "# Collect gradients (on CPU) for inspection\n",
        "ln_input_grads = {name: t.grad.detach().cpu() if t.grad is not None else None\n",
        "                  for name, t in ln_in_tensors.items()}\n",
        "\n",
        "print('Captured LN input tensors:', list(ln_in_tensors.keys()))\n",
        "print('Grad shapes summary:', {k: None if v is None else tuple(v.shape) for k, v in ln_input_grads.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient stats per LN input key (no mixing ln_1/ln_2). Heatmaps and per-key scalars\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "assert 'ln_input_grads' in globals(), \"Run the RMSNorm grad hook cell first.\"\n",
        "\n",
        "# Keep the original order of keys as captured\n",
        "keys = [k for k, v in ln_input_grads.items() if v is not None]\n",
        "if not keys:\n",
        "    raise RuntimeError('ln_input_grads has no populated entries with gradients.')\n",
        "\n",
        "means_KS = []  # (K, S)\n",
        "stds_KS = []   # (K, S)\n",
        "scalar_means_k = []\n",
        "scalar_stds_k = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for k in keys:\n",
        "        g = ln_input_grads[k]                 # numpy or tensor; expected shape (B, S, d)\n",
        "        g = torch.as_tensor(g, device=device)\n",
        "        mean_S = g.mean(dim=(0, 2)).detach().cpu().numpy()                 # (S,)\n",
        "        std_S  = g.std(dim=(0, 2), unbiased=False).detach().cpu().numpy()  # (S,)\n",
        "        means_KS.append(mean_S)\n",
        "        stds_KS.append(std_S)\n",
        "        scalar_means_k.append(float(g.mean().detach().cpu()))\n",
        "        scalar_stds_k.append(float(g.std(unbiased=False).detach().cpu()))\n",
        "\n",
        "means_KS = np.stack(means_KS, axis=0)  # (K, S)\n",
        "stds_KS  = np.stack(stds_KS,  axis=0)  # (K, S)\n",
        "K, S = means_KS.shape\n",
        "\n",
        "# Heatmap: gradient mean per key x token\n",
        "plt.figure(figsize=(10, max(4, 0.3*K + 3)))\n",
        "im1 = plt.imshow(means_KS, aspect='auto', cmap='viridis')\n",
        "plt.colorbar(im1, label='grad mean over batch+hidden')\n",
        "plt.xlabel('token position (0..S-1)')\n",
        "plt.ylabel('LN input key')\n",
        "plt.yticks(ticks=range(K), labels=keys)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('RMSNorm input gradient mean (per key x token)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: gradient std per key x token\n",
        "plt.figure(figsize=(10, max(4, 0.3*K + 3)))\n",
        "im2 = plt.imshow(stds_KS, aspect='auto', cmap='magma')\n",
        "plt.colorbar(im2, label='grad std over batch+hidden')\n",
        "plt.xlabel('token position (0..S-1)')\n",
        "plt.ylabel('LN input key')\n",
        "plt.yticks(ticks=range(K), labels=keys)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('RMSNorm input gradient std (per key x token)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scalar mean/std per key (over all dims)\n",
        "plt.figure(figsize=(max(7, 0.4*K + 3), 4))\n",
        "plt.plot(range(K), scalar_means_k, marker='o')\n",
        "plt.xticks(range(K), keys, rotation=45, ha='right')\n",
        "plt.xlabel('LN input key')\n",
        "plt.ylabel('grad mean over (B,S,d)')\n",
        "plt.title('RMSNorm input gradient mean vs key')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(max(7, 0.4*K + 3), 4))\n",
        "plt.plot(range(K), scalar_stds_k, marker='o', color='tab:orange')\n",
        "plt.xticks(range(K), keys, rotation=45, ha='right')\n",
        "plt.xlabel('LN input key')\n",
        "plt.ylabel('grad std over (B,S,d)')\n",
        "plt.title('RMSNorm input gradient std vs key')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache inputs to all RMSNorms (no grads), keyed by module name\n",
        "from collections import OrderedDict\n",
        "from model import RMSNorm as _RMSNorm\n",
        "\n",
        "rms_in_tensors = OrderedDict()\n",
        "hooks_in = []\n",
        "\n",
        "def _make_in_hook(name):\n",
        "    def hook(module, inp, out):\n",
        "        x_in = inp[0]\n",
        "        rms_in_tensors[name] = x_in.detach()\n",
        "    return hook\n",
        "\n",
        "# ln_emb (if post-LN)\n",
        "if hasattr(model.transformer, 'ln_emb') and isinstance(model.transformer.ln_emb, _RMSNorm):\n",
        "    hooks_in.append(model.transformer.ln_emb.register_forward_hook(_make_in_hook('ln_emb_in')))\n",
        "\n",
        "# per-block ln_1 / ln_2\n",
        "for li, block in enumerate(model.transformer.h):\n",
        "    if isinstance(block.ln_1, _RMSNorm):\n",
        "        hooks_in.append(block.ln_1.register_forward_hook(_make_in_hook(f'block{li}.ln_1_in')))\n",
        "    if isinstance(block.ln_2, _RMSNorm):\n",
        "        hooks_in.append(block.ln_2.register_forward_hook(_make_in_hook(f'block{li}.ln_2_in')))\n",
        "\n",
        "# final ln_f\n",
        "if isinstance(model.transformer.ln_f, _RMSNorm):\n",
        "    hooks_in.append(model.transformer.ln_f.register_forward_hook(_make_in_hook('ln_f_in')))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _ = model(Xb)  # use the same Xb you created earlier\n",
        "\n",
        "for h in hooks_in:\n",
        "    h.remove()\n",
        "\n",
        "print('Captured keys:', list(rms_in_tensors.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input stats per LN input key: heatmaps (mean/std over batch+hidden) and scalar per-key stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "keys_in = [k for k, v in rms_in_tensors.items() if v is not None]\n",
        "assert keys_in, \"No inputs captured; run the cache cell first.\"\n",
        "\n",
        "means_KS_in, stds_KS_in = [], []\n",
        "scalar_means_in, scalar_stds_in = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for k in keys_in:\n",
        "        x = rms_in_tensors[k].to(device)  # (B, S, d)\n",
        "        mean_S = x.mean(dim=(0, 2)).cpu().numpy()                 # (S,)\n",
        "        std_S  = x.std(dim=(0, 2), unbiased=False).cpu().numpy()  # (S,)\n",
        "        means_KS_in.append(mean_S)\n",
        "        stds_KS_in.append(std_S)\n",
        "        scalar_means_in.append(float(x.mean().cpu()))\n",
        "        scalar_stds_in.append(float(x.std(unbiased=False).cpu()))\n",
        "\n",
        "means_KS_in = np.stack(means_KS_in, axis=0)  # (K, S)\n",
        "stds_KS_in  = np.stack(stds_KS_in,  axis=0)  # (K, S)\n",
        "K, S = means_KS_in.shape\n",
        "\n",
        "# Heatmap: input mean\n",
        "plt.figure(figsize=(10, max(4, 0.3*K + 3)))\n",
        "im1 = plt.imshow(means_KS_in, aspect='auto', cmap='viridis')\n",
        "plt.colorbar(im1, label='mean over batch+hidden')\n",
        "plt.xlabel('token position (0..S-1)')\n",
        "plt.ylabel('LN input key')\n",
        "plt.yticks(ticks=range(K), labels=keys_in)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('RMSNorm input mean (per key x token)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: input std\n",
        "plt.figure(figsize=(10, max(4, 0.3*K + 3)))\n",
        "im2 = plt.imshow(stds_KS_in, aspect='auto', cmap='magma')\n",
        "plt.colorbar(im2, label='std over batch+hidden')\n",
        "plt.xlabel('token position (0..S-1)')\n",
        "plt.ylabel('LN input key')\n",
        "plt.yticks(ticks=range(K), labels=keys_in)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('RMSNorm input std (per key x token)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scalar mean/std per key\n",
        "plt.figure(figsize=(max(7, 0.4*K + 3), 4))\n",
        "plt.plot(range(K), scalar_means_in, marker='o')\n",
        "plt.xticks(range(K), keys_in, rotation=45, ha='right')\n",
        "plt.xlabel('LN input key')\n",
        "plt.ylabel('mean over (B,S,d)')\n",
        "plt.title('RMSNorm input mean vs key')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(max(7, 0.4*K + 3), 4))\n",
        "plt.plot(range(K), scalar_stds_in, marker='o', color='tab:orange')\n",
        "plt.xticks(range(K), keys_in, rotation=45, ha='right')\n",
        "plt.xlabel('LN input key')\n",
        "plt.ylabel('std over (B,S,d)')\n",
        "plt.title('RMSNorm input std vs key')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
